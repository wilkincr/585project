
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 2:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             2       
data parallel size:                                                     2       
model parallel size:                                                    1       
batch size per GPU:                                                     2       
params per GPU:                                                         496.24 M
params of model = params per GPU * mp_size:                             496.24 M
fwd MACs per GPU:                                                       1.15 TMACs
fwd flops per GPU:                                                      2.31 T  
fwd flops of model = fwd flops per GPU * mp_size:                       2.31 T  
fwd latency:                                                            468.26 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    4.92 TFLOPS
bwd latency:                                                            916.65 ms
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                5.03 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      4.99 TFLOPS
step latency:                                                           13.9 ms 
iter latency:                                                           1.4 s   
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   4.94 TFLOPS
samples/second:                                                         2.86    

----------------------------- Aggregated Profile per GPU -----------------------------
Top 3 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'PeftModel': '496.24 M'}
    MACs        - {'PeftModel': '1.15 TMACs'}
    fwd latency - {'PeftModel': '467.96 ms'}
depth 1:
    params      - {'LoraModel': '496.24 M'}
    MACs        - {'LoraModel': '1.15 TMACs'}
    fwd latency - {'LoraModel': '467.96 ms'}
depth 2:
    params      - {'CriticModel': '496.24 M'}
    MACs        - {'CriticModel': '1.15 TMACs'}
    fwd latency - {'CriticModel': '467.96 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

PeftModel(
  496.24 M = 100% Params, 1.15 TMACs = 100% MACs, 467.96 ms = 100% latency, 4.93 TFLOPS
  (base_model): LoraModel(
    496.24 M = 100% Params, 1.15 TMACs = 100% MACs, 467.96 ms = 100% latency, 4.93 TFLOPS
    (model): CriticModel(
      496.24 M = 100% Params, 1.15 TMACs = 100% MACs, 467.96 ms = 100% latency, 4.93 TFLOPS
      (model): Qwen2Model(
        496.23 M = 100% Params, 1.15 TMACs = 100% MACs, 466.7 ms = 99.73% latency, 4.94 TFLOPS
        (embed_tokens): Embedding(136.13 M = 27.43% Params, 0 MACs = 0% MACs, 162.84 us = 0.03% latency, 0 FLOPS, 151936, 896)
        (layers): ModuleList(
          (0): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 20.24 ms = 4.33% latency, 4.74 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 9.01 ms = 1.93% latency, 2.64 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.31 ms = 0.28% latency, 3.4 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 607.49 us = 0.13% latency, 7.26 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 277.52 us = 0.06% latency, 70.98 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 277.52 us = 0.06% latency, 70.98 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 152.59 us = 0.03% latency, 129.09 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 152.59 us = 0.03% latency, 129.09 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 889.54 us = 0.19% latency, 733.9 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 274.9 us = 0.06% latency, 2.29 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.15 us = 0.05% latency, 92.41 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.15 us = 0.05% latency, 92.41 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 154.26 us = 0.03% latency, 18.24 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 154.26 us = 0.03% latency, 18.24 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 821.35 us = 0.18% latency, 794.83 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 240.8 us = 0.05% latency, 2.62 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.72 us = 0.05% latency, 93.04 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.72 us = 0.05% latency, 93.04 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 126.84 us = 0.03% latency, 22.19 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 126.84 us = 0.03% latency, 22.19 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.16 ms = 0.25% latency, 3.82 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 550.75 us = 0.12% latency, 8.01 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 231.98 us = 0.05% latency, 84.91 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 231.98 us = 0.05% latency, 84.91 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.03 us = 0.03% latency, 153.85 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.03 us = 0.03% latency, 153.85 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 10.24 ms = 2.19% latency, 7.05 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.16 ms = 0.68% latency, 7.62 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.39 ms = 0.51% latency, 10.03 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 228.17 us = 0.05% latency, 86.33 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 228.17 us = 0.05% latency, 86.33 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 219.11 us = 0.05% latency, 488.03 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 219.11 us = 0.05% latency, 488.03 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.1 ms = 0.66% latency, 7.76 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.36 ms = 0.5% latency, 10.15 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.21 us = 0.05% latency, 86.69 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.21 us = 0.05% latency, 86.69 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.73 us = 0.04% latency, 560.62 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.73 us = 0.04% latency, 560.62 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.6 ms = 0.77% latency, 6.69 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.35 ms = 0.5% latency, 10.2 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 873.33 us = 0.19% latency, 122.44 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 873.33 us = 0.19% latency, 122.44 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.98 us = 0.03% latency, 152.71 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.98 us = 0.03% latency, 152.71 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 121.59 us = 0.03% latency, 109.93 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 441.31 us = 0.09% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 323.3 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (1): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 19.74 ms = 4.22% latency, 4.87 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.75 ms = 1.87% latency, 2.72 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.13 ms = 0.24% latency, 3.93 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 521.66 us = 0.11% latency, 8.46 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.45 us = 0.05% latency, 86.6 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.45 us = 0.05% latency, 86.6 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 136.38 us = 0.03% latency, 144.44 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 136.38 us = 0.03% latency, 144.44 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 881.2 us = 0.19% latency, 740.85 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 287.06 us = 0.06% latency, 2.2 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.77 us = 0.05% latency, 91.29 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.77 us = 0.05% latency, 91.29 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 132.08 us = 0.03% latency, 21.3 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 132.08 us = 0.03% latency, 21.3 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 835.9 us = 0.18% latency, 781 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 232.93 us = 0.05% latency, 2.71 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.95 us = 0.05% latency, 92.93 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.95 us = 0.05% latency, 92.93 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 130.41 us = 0.03% latency, 21.58 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 130.41 us = 0.03% latency, 21.58 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.13 ms = 0.24% latency, 3.93 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 525.95 us = 0.11% latency, 8.39 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.21 us = 0.05% latency, 86.69 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.21 us = 0.05% latency, 86.69 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.65 us = 0.03% latency, 156.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.65 us = 0.03% latency, 156.77 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 10.1 ms = 2.16% latency, 7.15 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.11 ms = 0.66% latency, 7.74 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.37 ms = 0.51% latency, 10.12 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.44 us = 0.05% latency, 88.55 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.44 us = 0.05% latency, 88.55 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 192.4 us = 0.04% latency, 555.76 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 192.4 us = 0.04% latency, 555.76 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.11 ms = 0.66% latency, 7.75 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.36 ms = 0.51% latency, 10.13 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 225.54 us = 0.05% latency, 87.33 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 225.54 us = 0.05% latency, 87.33 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.26 us = 0.04% latency, 562.03 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.26 us = 0.04% latency, 562.03 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.53 ms = 0.76% latency, 6.81 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.33 ms = 0.5% latency, 10.28 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 829.94 us = 0.18% latency, 128.84 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 829.94 us = 0.18% latency, 128.84 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 105.38 us = 0.02% latency, 126.84 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 337.84 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 333.79 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (2): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 19.71 ms = 4.21% latency, 4.87 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.73 ms = 1.87% latency, 2.73 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.13 ms = 0.24% latency, 3.94 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 519.75 us = 0.11% latency, 8.49 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.21 us = 0.05% latency, 86.69 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.21 us = 0.05% latency, 86.69 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 135.9 us = 0.03% latency, 144.94 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 135.9 us = 0.03% latency, 144.94 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 824.21 us = 0.18% latency, 792.07 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 235.8 us = 0.05% latency, 2.67 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.24 us = 0.05% latency, 93.25 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.24 us = 0.05% latency, 93.25 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 128.03 us = 0.03% latency, 21.98 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 128.03 us = 0.03% latency, 21.98 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 820.4 us = 0.18% latency, 795.76 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 231.98 us = 0.05% latency, 2.72 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 127.32 us = 0.03% latency, 22.1 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 127.32 us = 0.03% latency, 22.1 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.12 ms = 0.24% latency, 3.99 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 513.55 us = 0.11% latency, 8.59 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 226.74 us = 0.05% latency, 86.87 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 226.74 us = 0.05% latency, 86.87 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.6 us = 0.03% latency, 155.59 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.6 us = 0.03% latency, 155.59 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 10.09 ms = 2.16% latency, 7.16 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.11 ms = 0.66% latency, 7.74 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.36 ms = 0.5% latency, 10.14 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.68 us = 0.05% latency, 88.46 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.68 us = 0.05% latency, 88.46 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 200.75 us = 0.04% latency, 532.66 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 200.75 us = 0.04% latency, 532.66 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.1 ms = 0.66% latency, 7.77 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.36 ms = 0.5% latency, 10.16 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 223.16 us = 0.05% latency, 88.27 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 223.16 us = 0.05% latency, 88.27 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 191.45 us = 0.04% latency, 558.53 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 191.45 us = 0.04% latency, 558.53 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.53 ms = 0.75% latency, 6.82 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.33 ms = 0.5% latency, 10.3 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 828.03 us = 0.18% latency, 129.14 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 828.03 us = 0.18% latency, 129.14 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.69 us = 0.03% latency, 157.97 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.69 us = 0.03% latency, 157.97 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.67 us = 0.02% latency, 127.7 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 339.51 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 330.92 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (3): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 19.73 ms = 4.22% latency, 4.87 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.67 ms = 1.85% latency, 2.74 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.14 ms = 0.24% latency, 3.92 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 522.85 us = 0.11% latency, 8.44 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 229.84 us = 0.05% latency, 85.7 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 229.84 us = 0.05% latency, 85.7 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.99 us = 0.03% latency, 147.01 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.99 us = 0.03% latency, 147.01 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 829.22 us = 0.18% latency, 787.29 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 237.94 us = 0.05% latency, 2.65 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 132.56 us = 0.03% latency, 21.23 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 132.56 us = 0.03% latency, 21.23 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 840.43 us = 0.18% latency, 776.79 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 231.27 us = 0.05% latency, 2.73 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.43 us = 0.05% latency, 92.73 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.43 us = 0.05% latency, 92.73 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 150.92 us = 0.03% latency, 18.65 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 150.92 us = 0.03% latency, 18.65 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.11 ms = 0.24% latency, 3.99 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 512.6 us = 0.11% latency, 8.61 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.93 us = 0.05% latency, 86.42 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 227.93 us = 0.05% latency, 86.42 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.74 us = 0.03% latency, 159.19 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.74 us = 0.03% latency, 159.19 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 10.18 ms = 2.18% latency, 7.1 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.15 ms = 0.67% latency, 7.66 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.36 ms = 0.5% latency, 10.14 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.44 us = 0.05% latency, 88.55 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.44 us = 0.05% latency, 88.55 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 193.36 us = 0.04% latency, 553.02 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 193.36 us = 0.04% latency, 553.02 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.16 ms = 0.68% latency, 7.61 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.35 ms = 0.5% latency, 10.18 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 285.86 us = 0.06% latency, 68.91 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 285.86 us = 0.06% latency, 68.91 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 199.32 us = 0.04% latency, 536.48 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 199.32 us = 0.04% latency, 536.48 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.52 ms = 0.75% latency, 6.85 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.31 ms = 0.49% latency, 10.36 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 823.97 us = 0.18% latency, 129.77 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 823.97 us = 0.18% latency, 129.77 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.51 us = 0.03% latency, 153.28 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.51 us = 0.03% latency, 153.28 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 107.05 us = 0.02% latency, 124.86 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 333.79 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 332.83 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (4): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 19.59 ms = 4.19% latency, 4.9 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.67 ms = 1.85% latency, 2.74 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.12 ms = 0.24% latency, 3.96 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 518.32 us = 0.11% latency, 8.51 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 226.97 us = 0.05% latency, 86.78 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 226.97 us = 0.05% latency, 86.78 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 132.08 us = 0.03% latency, 149.13 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 132.08 us = 0.03% latency, 149.13 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 810.62 us = 0.17% latency, 805.35 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 234.13 us = 0.05% latency, 2.69 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 209.33 us = 0.04% latency, 94.1 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 209.33 us = 0.04% latency, 94.1 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.02 us = 0.03% latency, 22.87 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.02 us = 0.03% latency, 22.87 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 812.29 us = 0.17% latency, 803.7 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 231.98 us = 0.05% latency, 2.72 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.12 ms = 0.24% latency, 3.97 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 511.88 us = 0.11% latency, 8.62 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 229.6 us = 0.05% latency, 85.79 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 229.6 us = 0.05% latency, 85.79 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 10.04 ms = 2.15% latency, 7.19 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.09 ms = 0.66% latency, 7.8 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.35 ms = 0.5% latency, 10.18 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.68 us = 0.05% latency, 88.46 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.68 us = 0.05% latency, 88.46 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.97 us = 0.04% latency, 559.92 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.97 us = 0.04% latency, 559.92 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.1 ms = 0.66% latency, 7.77 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.36 ms = 0.5% latency, 10.15 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 224.35 us = 0.05% latency, 87.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 224.35 us = 0.05% latency, 87.8 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.97 us = 0.04% latency, 559.92 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 190.97 us = 0.04% latency, 559.92 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.51 ms = 0.75% latency, 6.86 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.31 ms = 0.49% latency, 10.36 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 827.55 us = 0.18% latency, 129.21 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 827.55 us = 0.18% latency, 129.21 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.9 us = 0.02% latency, 127.41 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 331.4 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (5): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.96 ms = 4.05% latency, 5.06 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.55 ms = 1.83% latency, 2.78 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.13 ms = 0.24% latency, 3.94 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 521.9 us = 0.11% latency, 8.45 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 228.17 us = 0.05% latency, 86.33 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 228.17 us = 0.05% latency, 86.33 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.17 us = 0.03% latency, 157.37 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.17 us = 0.03% latency, 157.37 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 829.22 us = 0.18% latency, 787.29 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 234.84 us = 0.05% latency, 2.68 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.72 us = 0.05% latency, 93.04 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.72 us = 0.05% latency, 93.04 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 140.19 us = 0.03% latency, 20.07 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 140.19 us = 0.03% latency, 20.07 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 868.32 us = 0.19% latency, 751.84 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 290.87 us = 0.06% latency, 2.17 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211 us = 0.05% latency, 93.35 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211 us = 0.05% latency, 93.35 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 125.41 us = 0.03% latency, 22.44 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 125.41 us = 0.03% latency, 22.44 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.12 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 486.85 us = 0.1% latency, 9.06 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.68 us = 0.05% latency, 90.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.68 us = 0.05% latency, 90.49 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.69 us = 0.03% latency, 157.97 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.69 us = 0.03% latency, 157.97 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.53 ms = 2.04% latency, 7.58 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.19 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.8 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.05 us = 0.05% latency, 91.59 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.05 us = 0.05% latency, 91.59 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 184.54 us = 0.04% latency, 579.45 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 184.54 us = 0.04% latency, 579.45 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.83 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.38 us = 0.05% latency, 92.31 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.38 us = 0.05% latency, 92.31 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 184.3 us = 0.04% latency, 580.2 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 184.3 us = 0.04% latency, 580.2 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.31 ms = 0.71% latency, 7.27 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.17 ms = 0.46% latency, 11.06 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 778.91 us = 0.17% latency, 137.28 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 778.91 us = 0.17% latency, 137.28 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.45 us = 0.03% latency, 158.27 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.45 us = 0.03% latency, 158.27 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 106.1 us = 0.02% latency, 125.98 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 338.08 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 327.83 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (6): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 19.73 ms = 4.22% latency, 4.87 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.38 ms = 1.79% latency, 2.84 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.13 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 490.19 us = 0.1% latency, 9 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 26.23 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 26.23 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.11 us = 0.05% latency, 89.9 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.11 us = 0.05% latency, 89.9 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.98 us = 0.03% latency, 158.88 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.98 us = 0.03% latency, 158.88 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 812.05 us = 0.17% latency, 803.93 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 226.74 us = 0.05% latency, 2.78 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 204.09 us = 0.04% latency, 96.52 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 204.09 us = 0.04% latency, 96.52 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 800.61 us = 0.17% latency, 815.42 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 229.36 us = 0.05% latency, 2.75 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.18 us = 0.04% latency, 97.43 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.18 us = 0.04% latency, 97.43 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.1 ms = 0.24% latency, 4.05 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 493.76 us = 0.11% latency, 8.94 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 221.01 us = 0.05% latency, 89.12 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 221.01 us = 0.05% latency, 89.12 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.32 us = 0.03% latency, 154.72 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.32 us = 0.03% latency, 154.72 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 10.47 ms = 2.24% latency, 6.9 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.19 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.81 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 184.06 us = 0.04% latency, 580.96 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 184.06 us = 0.04% latency, 580.96 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.85 ms = 0.82% latency, 6.25 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.81 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 1.12 ms = 0.24% latency, 17.51 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 1.12 ms = 0.24% latency, 17.51 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.25 us = 0.04% latency, 577.22 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.25 us = 0.04% latency, 577.22 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.33 ms = 0.71% latency, 7.22 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.18 ms = 0.47% latency, 10.99 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 779.39 us = 0.17% latency, 137.2 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 779.39 us = 0.17% latency, 137.2 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.6 us = 0.03% latency, 155.59 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.6 us = 0.03% latency, 155.59 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.9 us = 0.02% latency, 127.41 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 329.97 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 323.3 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (7): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.87 ms = 4.03% latency, 5.09 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.41 ms = 1.8% latency, 2.83 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.07 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 498.53 us = 0.11% latency, 8.85 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.58 us = 0.05% latency, 89.7 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.58 us = 0.05% latency, 89.7 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.84 us = 0.03% latency, 155.3 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.84 us = 0.03% latency, 155.3 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 817.78 us = 0.17% latency, 798.31 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 236.51 us = 0.05% latency, 2.67 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 133.99 us = 0.03% latency, 21 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 133.99 us = 0.03% latency, 21 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 801.09 us = 0.17% latency, 814.94 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 223.16 us = 0.05% latency, 2.82 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.7 us = 0.04% latency, 97.66 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.7 us = 0.04% latency, 97.66 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.12 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 485.9 us = 0.1% latency, 9.08 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.58 us = 0.05% latency, 89.7 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.58 us = 0.05% latency, 89.7 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.74 us = 0.03% latency, 159.19 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.74 us = 0.03% latency, 159.19 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.6 ms = 2.05% latency, 7.53 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.2 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.82 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.62 us = 0.05% latency, 92.21 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.62 us = 0.05% latency, 92.21 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.67 us = 0.04% latency, 588.58 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.67 us = 0.04% latency, 588.58 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.92 ms = 0.62% latency, 8.24 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.86 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.95 us = 0.05% latency, 92.93 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.95 us = 0.05% latency, 92.93 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.87 us = 0.04% latency, 584.74 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.87 us = 0.04% latency, 584.74 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.39 ms = 0.72% latency, 7.1 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.18 ms = 0.47% latency, 11 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 842.33 us = 0.18% latency, 126.95 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 842.33 us = 0.18% latency, 126.95 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.9 us = 0.02% latency, 127.41 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 317.81 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (8): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.74 ms = 4% latency, 5.12 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.36 ms = 1.79% latency, 2.85 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.12 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 489.71 us = 0.1% latency, 9.01 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.68 us = 0.05% latency, 90.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.68 us = 0.05% latency, 90.49 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.12 us = 0.03% latency, 156.18 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.12 us = 0.03% latency, 156.18 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 797.99 us = 0.17% latency, 818.1 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 226.5 us = 0.05% latency, 2.78 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.89 us = 0.04% latency, 97.08 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.89 us = 0.04% latency, 97.08 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.5 us = 0.03% latency, 22.78 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.5 us = 0.03% latency, 22.78 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 803.71 us = 0.17% latency, 812.28 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 221.25 us = 0.05% latency, 2.85 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.7 us = 0.04% latency, 97.66 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.7 us = 0.04% latency, 97.66 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 126.84 us = 0.03% latency, 22.19 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 126.84 us = 0.03% latency, 22.19 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.13 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 485.9 us = 0.1% latency, 9.08 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.96 us = 0.05% latency, 90.79 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.96 us = 0.05% latency, 90.79 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.52 ms = 2.03% latency, 7.59 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.82 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.15 us = 0.05% latency, 92.41 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.15 us = 0.05% latency, 92.41 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.63 us = 0.04% latency, 585.51 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.63 us = 0.04% latency, 585.51 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.23 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.83 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.15 us = 0.04% latency, 587.04 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.15 us = 0.04% latency, 587.04 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.31 ms = 0.71% latency, 7.27 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.17 ms = 0.46% latency, 11.05 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 778.2 us = 0.17% latency, 137.41 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 778.2 us = 0.17% latency, 137.41 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 103.24 us = 0.02% latency, 129.47 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 326.63 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 321.15 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (9): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.87 ms = 4.03% latency, 5.09 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.44 ms = 1.8% latency, 2.82 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.1 ms = 0.24% latency, 4.04 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 492.33 us = 0.11% latency, 8.96 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 231.03 us = 0.05% latency, 85.26 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 231.03 us = 0.05% latency, 85.26 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.98 us = 0.03% latency, 158.88 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.98 us = 0.03% latency, 158.88 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 785.83 us = 0.17% latency, 830.76 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 225.07 us = 0.05% latency, 2.8 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.03 us = 0.04% latency, 98.47 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.03 us = 0.04% latency, 98.47 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 122.07 us = 0.03% latency, 23.05 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 122.07 us = 0.03% latency, 23.05 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 789.4 us = 0.17% latency, 827 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 221.01 us = 0.05% latency, 2.85 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.66 us = 0.04% latency, 97.2 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.66 us = 0.04% latency, 97.2 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 125.65 us = 0.03% latency, 22.4 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 125.65 us = 0.03% latency, 22.4 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.11 ms = 0.24% latency, 4.01 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 481.84 us = 0.1% latency, 9.16 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.11 us = 0.05% latency, 89.9 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.11 us = 0.05% latency, 89.9 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.89 us = 0.03% latency, 156.47 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.52 ms = 2.04% latency, 7.59 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.19 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.81 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.1 us = 0.05% latency, 92 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.1 us = 0.05% latency, 92 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.25 us = 0.04% latency, 577.22 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.25 us = 0.04% latency, 577.22 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.2 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.79 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.3 ms = 0.71% latency, 7.29 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.16 ms = 0.46% latency, 11.1 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.96 us = 0.17% latency, 137.45 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.96 us = 0.17% latency, 137.45 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.5 us = 0.03% latency, 159.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.5 us = 0.03% latency, 159.49 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.02% latency, 128 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 374.32 us = 0.08% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 314.24 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (10): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.8 ms = 4.02% latency, 5.11 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.35 ms = 1.78% latency, 2.85 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.11 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 489.95 us = 0.1% latency, 9.01 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 26.46 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 26.46 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.3 us = 0.05% latency, 89.41 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.3 us = 0.05% latency, 89.41 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.45 us = 0.03% latency, 158.27 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.45 us = 0.03% latency, 158.27 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 796.56 us = 0.17% latency, 819.57 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 227.69 us = 0.05% latency, 2.77 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.22 us = 0.03% latency, 22.65 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 799.18 us = 0.17% latency, 816.88 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 223.64 us = 0.05% latency, 2.82 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 199.32 us = 0.04% latency, 98.83 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 199.32 us = 0.04% latency, 98.83 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 133.51 us = 0.03% latency, 21.08 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 133.51 us = 0.03% latency, 21.08 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.12 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 484.7 us = 0.1% latency, 9.1 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.96 us = 0.05% latency, 90.79 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.96 us = 0.05% latency, 90.79 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.58 ms = 2.05% latency, 7.54 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.2 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.79 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.91 us = 0.05% latency, 92.52 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.91 us = 0.05% latency, 92.52 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.63 us = 0.04% latency, 585.51 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.63 us = 0.04% latency, 585.51 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.22 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.83 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.91 us = 0.05% latency, 92.52 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.91 us = 0.05% latency, 92.52 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.58 us = 0.04% latency, 582.46 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.58 us = 0.04% latency, 582.46 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.36 ms = 0.72% latency, 7.16 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.2 ms = 0.47% latency, 10.87 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 781.3 us = 0.17% latency, 136.86 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 781.3 us = 0.17% latency, 136.86 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.04 us = 0.03% latency, 148.06 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.04 us = 0.03% latency, 148.06 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.67 us = 0.02% latency, 127.7 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 332.59 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 325.44 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (11): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.84 ms = 4.03% latency, 5.1 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.41 ms = 1.8% latency, 2.83 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.11 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 490.19 us = 0.1% latency, 9 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.08 us = 0.03% latency, 155.01 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.08 us = 0.03% latency, 155.01 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 800.13 us = 0.17% latency, 815.91 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 223.16 us = 0.05% latency, 2.82 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 33.14 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 33.14 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.98 us = 0.03% latency, 22.7 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.98 us = 0.03% latency, 22.7 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 791.31 us = 0.17% latency, 825.01 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 221.73 us = 0.05% latency, 2.84 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.99 us = 0.04% latency, 98 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.99 us = 0.04% latency, 98 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 122.55 us = 0.03% latency, 22.96 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 122.55 us = 0.03% latency, 22.96 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.08 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 483.04 us = 0.1% latency, 9.13 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.39 us = 0.05% latency, 90.19 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.39 us = 0.05% latency, 90.19 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 134.23 us = 0.03% latency, 146.75 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 134.23 us = 0.03% latency, 146.75 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.57 ms = 2.05% latency, 7.55 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.96 ms = 0.63% latency, 8.13 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.8 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.05 us = 0.05% latency, 91.59 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.05 us = 0.05% latency, 91.59 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.01 us = 0.04% latency, 577.96 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.01 us = 0.04% latency, 577.96 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.82 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.38 us = 0.05% latency, 92.31 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.38 us = 0.05% latency, 92.31 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.67 us = 0.04% latency, 588.58 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.67 us = 0.04% latency, 588.58 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.32 ms = 0.71% latency, 7.24 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.17 ms = 0.46% latency, 11.01 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.01 us = 0.17% latency, 137.62 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.01 us = 0.17% latency, 137.62 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 106.1 us = 0.02% latency, 125.98 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 341.18 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 313.28 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (12): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.89 ms = 4.04% latency, 5.08 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.37 ms = 1.79% latency, 2.84 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.09 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 488.28 us = 0.1% latency, 9.04 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.25 us = 0.05% latency, 91.09 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.25 us = 0.05% latency, 91.09 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.28 us = 0.03% latency, 147.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.28 us = 0.03% latency, 147.8 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 792.74 us = 0.17% latency, 823.52 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 228.4 us = 0.05% latency, 2.76 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.02 us = 0.03% latency, 22.87 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.02 us = 0.03% latency, 22.87 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 796.32 us = 0.17% latency, 819.82 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 220.3 us = 0.05% latency, 2.86 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 32.42 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 32.42 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.5 us = 0.03% latency, 22.78 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.5 us = 0.03% latency, 22.78 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.12 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 483.75 us = 0.1% latency, 9.12 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.68 us = 0.05% latency, 90.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.68 us = 0.05% latency, 90.49 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.93 us = 0.03% latency, 157.67 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.93 us = 0.03% latency, 157.67 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.59 ms = 2.05% latency, 7.53 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.99 ms = 0.64% latency, 8.04 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.8 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.29 us = 0.05% latency, 91.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.29 us = 0.05% latency, 91.49 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.85 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.67 us = 0.05% latency, 92.62 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.67 us = 0.05% latency, 92.62 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.32 ms = 0.71% latency, 7.25 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.16 ms = 0.46% latency, 11.09 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 776.29 us = 0.17% latency, 137.74 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 776.29 us = 0.17% latency, 137.74 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 131.13 us = 0.03% latency, 150.21 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 131.13 us = 0.03% latency, 150.21 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.02% latency, 128 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 396.01 us = 0.08% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 320.91 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (13): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.83 ms = 4.02% latency, 5.1 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.39 ms = 1.79% latency, 2.84 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.13 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 489 us = 0.1% latency, 9.02 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.15 us = 0.05% latency, 90.29 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.15 us = 0.05% latency, 90.29 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.45 us = 0.03% latency, 158.27 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.45 us = 0.03% latency, 158.27 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 792.74 us = 0.17% latency, 823.52 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 224.83 us = 0.05% latency, 2.8 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 199.32 us = 0.04% latency, 98.83 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 199.32 us = 0.04% latency, 98.83 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 125.65 us = 0.03% latency, 22.4 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 125.65 us = 0.03% latency, 22.4 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 798.46 us = 0.17% latency, 817.62 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 222.21 us = 0.05% latency, 2.84 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 209.09 us = 0.04% latency, 94.21 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 209.09 us = 0.04% latency, 94.21 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.26 us = 0.03% latency, 22.83 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.26 us = 0.03% latency, 22.83 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.07 ms = 0.23% latency, 4.16 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 483.04 us = 0.1% latency, 9.13 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.53 us = 0.05% latency, 91.39 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.53 us = 0.05% latency, 91.39 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.93 us = 0.03% latency, 157.67 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.93 us = 0.03% latency, 157.67 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.57 ms = 2.04% latency, 7.55 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.19 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.82 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.22 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.83 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.44 us = 0.04% latency, 589.35 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.44 us = 0.04% latency, 589.35 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.35 ms = 0.72% latency, 7.19 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.16 ms = 0.46% latency, 11.07 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 785.35 us = 0.17% latency, 136.16 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 785.35 us = 0.17% latency, 136.16 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 106.1 us = 0.02% latency, 125.98 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 340.22 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 317.57 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (14): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.83 ms = 4.02% latency, 5.1 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.37 ms = 1.79% latency, 2.84 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.1 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 497.58 us = 0.11% latency, 8.87 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.39 us = 0.05% latency, 90.19 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.39 us = 0.05% latency, 90.19 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.17 us = 0.03% latency, 157.37 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.17 us = 0.03% latency, 157.37 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 794.89 us = 0.17% latency, 821.29 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 227.45 us = 0.05% latency, 2.77 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.75 us = 0.04% latency, 98.12 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.75 us = 0.04% latency, 98.12 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 791.79 us = 0.17% latency, 824.51 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 225.07 us = 0.05% latency, 2.8 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.23 us = 0.04% latency, 97.89 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.23 us = 0.04% latency, 97.89 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.5 us = 0.03% latency, 22.78 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.5 us = 0.03% latency, 22.78 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.1 ms = 0.24% latency, 4.04 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 485.66 us = 0.1% latency, 9.09 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 226.26 us = 0.05% latency, 87.06 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 226.26 us = 0.05% latency, 87.06 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.03 us = 0.03% latency, 153.85 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.03 us = 0.03% latency, 153.85 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.59 ms = 2.05% latency, 7.53 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.99 ms = 0.64% latency, 8.06 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.27 ms = 0.49% latency, 10.55 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.92 ms = 0.62% latency, 8.24 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.85 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.91 us = 0.05% latency, 92.52 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.91 us = 0.05% latency, 92.52 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.34 ms = 0.71% latency, 7.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.18 ms = 0.46% latency, 11.01 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 780.11 us = 0.17% latency, 137.07 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 780.11 us = 0.17% latency, 137.07 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 137.09 us = 0.03% latency, 143.68 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 137.09 us = 0.03% latency, 143.68 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 103.95 us = 0.02% latency, 128.58 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 332.59 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 319.48 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (15): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.73 ms = 4% latency, 5.13 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.34 ms = 1.78% latency, 2.85 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.09 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 488.76 us = 0.1% latency, 9.03 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 225.31 us = 0.05% latency, 87.43 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 225.31 us = 0.05% latency, 87.43 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 130.65 us = 0.03% latency, 150.76 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 130.65 us = 0.03% latency, 150.76 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 794.41 us = 0.17% latency, 821.79 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 227.45 us = 0.05% latency, 2.77 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.51 us = 0.04% latency, 98.24 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.51 us = 0.04% latency, 98.24 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 125.41 us = 0.03% latency, 22.44 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 125.41 us = 0.03% latency, 22.44 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 794.89 us = 0.17% latency, 821.29 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 222.44 us = 0.05% latency, 2.83 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.99 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.75 us = 0.04% latency, 98.12 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.75 us = 0.04% latency, 98.12 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 127.08 us = 0.03% latency, 22.14 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 127.08 us = 0.03% latency, 22.14 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.13 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 480.65 us = 0.1% latency, 9.18 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 223.4 us = 0.05% latency, 88.17 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 223.4 us = 0.05% latency, 88.17 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.5 us = 0.03% latency, 159.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.5 us = 0.03% latency, 159.49 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.53 ms = 2.04% latency, 7.58 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.2 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.84 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.67 us = 0.05% latency, 92.62 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.67 us = 0.05% latency, 92.62 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 189.78 us = 0.04% latency, 563.44 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 189.78 us = 0.04% latency, 563.44 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.22 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.84 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.95 us = 0.05% latency, 92.93 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.95 us = 0.05% latency, 92.93 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.39 us = 0.04% latency, 586.27 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.32 ms = 0.71% latency, 7.26 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.17 ms = 0.46% latency, 11.02 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 776.29 us = 0.17% latency, 137.74 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 776.29 us = 0.17% latency, 137.74 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.5 us = 0.03% latency, 159.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.5 us = 0.03% latency, 159.49 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.02% latency, 128 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 319 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (16): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.84 ms = 4.03% latency, 5.1 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.44 ms = 1.8% latency, 2.82 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.16 ms = 0.25% latency, 3.82 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 513.08 us = 0.11% latency, 8.6 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.48 us = 0.05% latency, 90.99 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.48 us = 0.05% latency, 90.99 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 189.78 us = 0.04% latency, 103.79 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 189.78 us = 0.04% latency, 103.79 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 804.66 us = 0.17% latency, 811.32 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 227.93 us = 0.05% latency, 2.77 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.93 us = 0.03% latency, 22.52 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.93 us = 0.03% latency, 22.52 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 802.04 us = 0.17% latency, 813.97 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 234.37 us = 0.05% latency, 2.69 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.51 us = 0.04% latency, 98.24 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.51 us = 0.04% latency, 98.24 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.26 us = 0.03% latency, 22.83 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.26 us = 0.03% latency, 22.83 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.07 ms = 0.23% latency, 4.15 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 483.04 us = 0.1% latency, 9.13 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.01 us = 0.05% latency, 91.19 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.01 us = 0.05% latency, 91.19 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.93 us = 0.03% latency, 157.67 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.93 us = 0.03% latency, 157.67 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.54 ms = 2.04% latency, 7.57 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.83 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.34 us = 0.05% latency, 91.9 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.34 us = 0.05% latency, 91.9 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.58 us = 0.04% latency, 582.46 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.58 us = 0.04% latency, 582.46 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.82 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.67 us = 0.05% latency, 92.62 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.67 us = 0.05% latency, 92.62 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.44 us = 0.04% latency, 589.35 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.44 us = 0.04% latency, 589.35 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.33 ms = 0.71% latency, 7.23 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.18 ms = 0.47% latency, 10.97 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 779.15 us = 0.17% latency, 137.24 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 779.15 us = 0.17% latency, 137.24 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.02% latency, 128 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 319.72 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (17): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.78 ms = 4.01% latency, 5.11 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.4 ms = 1.8% latency, 2.83 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.13 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 492.57 us = 0.11% latency, 8.96 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.72 us = 0.05% latency, 90.89 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.72 us = 0.05% latency, 90.89 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 790.83 us = 0.17% latency, 825.5 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 225.54 us = 0.05% latency, 2.79 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.46 us = 0.04% latency, 97.77 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 122.55 us = 0.03% latency, 22.96 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 122.55 us = 0.03% latency, 22.96 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 789.64 us = 0.17% latency, 826.75 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 223.64 us = 0.05% latency, 2.82 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 199.08 us = 0.04% latency, 98.94 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 199.08 us = 0.04% latency, 98.94 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.93 us = 0.03% latency, 22.52 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.93 us = 0.03% latency, 22.52 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.13 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 481.37 us = 0.1% latency, 9.17 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.48 us = 0.05% latency, 90.99 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.48 us = 0.05% latency, 90.99 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 131.85 us = 0.03% latency, 149.4 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 131.85 us = 0.03% latency, 149.4 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.52 ms = 2.03% latency, 7.59 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.21 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.8 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.43 us = 0.05% latency, 92.73 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.43 us = 0.05% latency, 92.73 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.91 us = 0.04% latency, 587.81 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.91 us = 0.04% latency, 587.81 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.92 ms = 0.62% latency, 8.24 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.84 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211 us = 0.05% latency, 93.35 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211 us = 0.05% latency, 93.35 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.87 us = 0.04% latency, 584.74 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.87 us = 0.04% latency, 584.74 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.31 ms = 0.71% latency, 7.28 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.16 ms = 0.46% latency, 11.08 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 780.34 us = 0.17% latency, 137.03 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 780.34 us = 0.17% latency, 137.03 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 122.31 us = 0.03% latency, 161.05 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 122.31 us = 0.03% latency, 161.05 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 103.71 us = 0.02% latency, 128.88 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 333.07 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 314.95 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (18): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.87 ms = 4.03% latency, 5.09 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.35 ms = 1.78% latency, 2.85 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.12 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 497.58 us = 0.11% latency, 8.87 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.03 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.77 us = 0.05% latency, 91.29 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.77 us = 0.05% latency, 91.29 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.26 us = 0.03% latency, 159.8 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 820.16 us = 0.18% latency, 795.99 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 227.69 us = 0.05% latency, 2.77 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.89 us = 0.04% latency, 97.08 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.89 us = 0.04% latency, 97.08 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 149.73 us = 0.03% latency, 18.79 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 149.73 us = 0.03% latency, 18.79 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 787.26 us = 0.17% latency, 829.25 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 224.11 us = 0.05% latency, 2.81 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.23 us = 0.04% latency, 97.89 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.23 us = 0.04% latency, 97.89 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 121.59 us = 0.03% latency, 23.14 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 121.59 us = 0.03% latency, 23.14 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.12 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 481.13 us = 0.1% latency, 9.17 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.01 us = 0.05% latency, 91.19 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 216.01 us = 0.05% latency, 91.19 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.98 us = 0.03% latency, 158.88 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 123.98 us = 0.03% latency, 158.88 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.62 ms = 2.06% latency, 7.51 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.95 ms = 0.63% latency, 8.16 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.79 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 223.88 us = 0.05% latency, 87.99 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 223.88 us = 0.05% latency, 87.99 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.87 us = 0.04% latency, 584.74 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.87 us = 0.04% latency, 584.74 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3 ms = 0.64% latency, 8.03 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.27 ms = 0.49% latency, 10.55 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 224.35 us = 0.05% latency, 87.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 224.35 us = 0.05% latency, 87.8 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.15 us = 0.04% latency, 587.04 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.15 us = 0.04% latency, 587.04 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.32 ms = 0.71% latency, 7.25 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.17 ms = 0.46% latency, 11.02 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 779.15 us = 0.17% latency, 137.24 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 779.15 us = 0.17% latency, 137.24 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 122.31 us = 0.03% latency, 161.05 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 122.31 us = 0.03% latency, 161.05 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 111.58 us = 0.02% latency, 119.79 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 379.56 us = 0.08% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 311.61 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (19): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.76 ms = 4.01% latency, 5.12 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.32 ms = 1.78% latency, 2.86 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.11 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 488.76 us = 0.1% latency, 9.03 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.68 us = 0.05% latency, 88.46 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 222.68 us = 0.05% latency, 88.46 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 795.84 us = 0.17% latency, 820.31 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 224.59 us = 0.05% latency, 2.81 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 198.36 us = 0.04% latency, 99.3 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 198.36 us = 0.04% latency, 99.3 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 131.13 us = 0.03% latency, 21.46 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 131.13 us = 0.03% latency, 21.46 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 787.26 us = 0.17% latency, 829.25 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 223.88 us = 0.05% latency, 2.82 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.51 us = 0.04% latency, 98.24 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 200.51 us = 0.04% latency, 98.24 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.74 us = 0.03% latency, 22.74 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.74 us = 0.03% latency, 22.74 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.14 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 481.61 us = 0.1% latency, 9.16 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.44 us = 0.05% latency, 90.59 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.44 us = 0.05% latency, 90.59 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.69 us = 0.03% latency, 157.97 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.69 us = 0.03% latency, 157.97 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.57 ms = 2.04% latency, 7.55 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.2 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.79 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.91 us = 0.04% latency, 587.81 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.91 us = 0.04% latency, 587.81 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.93 ms = 0.63% latency, 8.23 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.84 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.32 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211 us = 0.05% latency, 93.35 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211 us = 0.05% latency, 93.35 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.2 us = 0.04% latency, 590.13 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 181.2 us = 0.04% latency, 590.13 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.36 ms = 0.72% latency, 7.18 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.84 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 778.68 us = 0.17% latency, 137.32 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 778.68 us = 0.17% latency, 137.32 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 122.55 us = 0.03% latency, 160.74 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 122.55 us = 0.03% latency, 160.74 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 103.95 us = 0.02% latency, 128.58 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 333.31 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (20): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.89 ms = 4.04% latency, 5.08 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.48 ms = 1.81% latency, 2.81 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.07 ms = 0.23% latency, 4.17 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 489.23 us = 0.1% latency, 9.02 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.29 us = 0.05% latency, 91.49 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.29 us = 0.05% latency, 91.49 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 121.83 us = 0.03% latency, 161.68 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 121.83 us = 0.03% latency, 161.68 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 861.64 us = 0.18% latency, 757.66 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 224.35 us = 0.05% latency, 2.81 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 198.84 us = 0.04% latency, 99.06 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 198.84 us = 0.04% latency, 99.06 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 186.92 us = 0.04% latency, 15.05 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 186.92 us = 0.04% latency, 15.05 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 786.54 us = 0.17% latency, 830.01 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 220.78 us = 0.05% latency, 2.86 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 204.09 us = 0.04% latency, 96.52 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 204.09 us = 0.04% latency, 96.52 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 121.12 us = 0.03% latency, 23.23 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 121.12 us = 0.03% latency, 23.23 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.08 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 493.29 us = 0.11% latency, 8.94 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.98 us = 0.03% latency, 152.71 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 128.98 us = 0.03% latency, 152.71 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.54 ms = 2.04% latency, 7.57 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.19 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.79 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 211.48 us = 0.05% latency, 93.14 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.2 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.82 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.05 us = 0.05% latency, 91.59 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.05 us = 0.05% latency, 91.59 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.15 us = 0.04% latency, 587.04 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 182.15 us = 0.04% latency, 587.04 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.32 ms = 0.71% latency, 7.25 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.17 ms = 0.46% latency, 11.04 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.48 us = 0.17% latency, 137.53 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.48 us = 0.17% latency, 137.53 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.32 us = 0.03% latency, 154.72 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.32 us = 0.03% latency, 154.72 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.67 us = 0.02% latency, 127.7 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 329.49 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 327.59 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (21): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.88 ms = 4.03% latency, 5.09 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.39 ms = 1.79% latency, 2.83 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.08 ms = 0.23% latency, 4.13 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 488.04 us = 0.1% latency, 9.04 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.63 us = 0.05% latency, 90.1 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 218.63 us = 0.05% latency, 90.1 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 125.41 us = 0.03% latency, 157.07 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 807.29 us = 0.17% latency, 808.68 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 227.69 us = 0.05% latency, 2.77 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.08 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 212.19 us = 0.05% latency, 92.83 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 125.17 us = 0.03% latency, 22.48 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 125.17 us = 0.03% latency, 22.48 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 801.56 us = 0.17% latency, 814.45 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 223.88 us = 0.05% latency, 2.82 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 26.46 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 26.46 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.23 us = 0.04% latency, 97.89 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 201.23 us = 0.04% latency, 97.89 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 130.65 us = 0.03% latency, 21.54 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 130.65 us = 0.03% latency, 21.54 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.1 ms = 0.23% latency, 4.06 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 486.85 us = 0.1% latency, 9.06 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 221.73 us = 0.05% latency, 88.84 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 221.73 us = 0.05% latency, 88.84 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.36 us = 0.03% latency, 155.88 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 126.36 us = 0.03% latency, 155.88 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.6 ms = 2.05% latency, 7.53 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.99 ms = 0.64% latency, 8.05 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.78 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.2 us = 0.05% latency, 90.69 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.2 us = 0.05% latency, 90.69 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.11 us = 0.04% latency, 583.98 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.19 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.83 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.82 us = 0.05% latency, 91.7 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.82 us = 0.05% latency, 91.7 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 187.64 us = 0.04% latency, 569.88 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 187.64 us = 0.04% latency, 569.88 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.32 ms = 0.71% latency, 7.25 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.17 ms = 0.46% latency, 11.02 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.24 us = 0.17% latency, 137.58 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 777.24 us = 0.17% latency, 137.58 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 108.24 us = 0.02% latency, 123.49 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 343.08 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 329.26 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (22): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.96 ms = 4.05% latency, 5.07 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.45 ms = 1.81% latency, 2.82 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.1 ms = 0.23% latency, 4.06 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 494.96 us = 0.11% latency, 8.91 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 220.06 us = 0.05% latency, 89.51 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 130.41 us = 0.03% latency, 151.04 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 130.41 us = 0.03% latency, 151.04 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 814.2 us = 0.17% latency, 801.81 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 223.64 us = 0.05% latency, 2.82 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.91 us = 0.05% latency, 90.39 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 217.91 us = 0.05% latency, 90.39 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 799.18 us = 0.17% latency, 816.88 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 224.83 us = 0.05% latency, 2.8 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.56 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 207.19 us = 0.04% latency, 95.07 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 207.19 us = 0.04% latency, 95.07 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 123.74 us = 0.03% latency, 22.74 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 123.74 us = 0.03% latency, 22.74 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.1 ms = 0.23% latency, 4.06 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 495.43 us = 0.11% latency, 8.91 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 221.49 us = 0.05% latency, 88.93 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 221.49 us = 0.05% latency, 88.93 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.63 ms = 2.06% latency, 7.5 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.95 ms = 0.63% latency, 8.17 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.83 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.77 us = 0.05% latency, 91.29 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 215.77 us = 0.05% latency, 91.29 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 186.68 us = 0.04% latency, 572.79 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 186.68 us = 0.04% latency, 572.79 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.18 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.81 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 213.86 us = 0.05% latency, 92.1 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 191.93 us = 0.04% latency, 557.14 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 191.93 us = 0.04% latency, 557.14 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.39 ms = 0.72% latency, 7.1 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.23 ms = 0.48% latency, 10.74 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 784.64 us = 0.17% latency, 136.28 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 784.64 us = 0.17% latency, 136.28 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.08 us = 0.03% latency, 155.01 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 127.08 us = 0.03% latency, 155.01 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.67 us = 0.02% latency, 127.7 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 331.88 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 325.2 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
          (23): Qwen2DecoderLayer(
            15 M = 3.02% Params, 47.99 GMACs = 4.17% MACs, 18.84 ms = 4.03% latency, 5.1 TFLOPS
            (self_attn): Qwen2Attention(
              1.86 M = 0.37% Params, 11.87 GMACs = 1.03% MACs, 8.43 ms = 1.8% latency, 2.82 TFLOPS
              (q_proj): lora.Linear(
                810.88 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.07 TFLOPS
                (base_layer): Linear(803.71 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 492.57 us = 0.11% latency, 8.96 TFLOPS, in_features=896, out_features=896, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.27 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.58 us = 0.05% latency, 91.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.58 us = 0.05% latency, 91.8 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.04 us = 0.03% latency, 148.06 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 133.04 us = 0.03% latency, 148.06 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (k_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 813.48 us = 0.17% latency, 802.52 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 232.22 us = 0.05% latency, 2.71 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 210.76 us = 0.05% latency, 93.46 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 210.76 us = 0.05% latency, 93.46 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 128.98 us = 0.03% latency, 21.82 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 128.98 us = 0.03% latency, 21.82 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (v_proj): lora.Linear(
                118.91 K = 0.02% Params, 326.42 MMACs = 0.03% MACs, 800.13 us = 0.17% latency, 815.91 GFLOPS
                (base_layer): Linear(114.82 K = 0.02% Params, 315.16 MMACs = 0.03% MACs, 227.45 us = 0.05% latency, 2.77 TFLOPS, in_features=896, out_features=128, bias=True)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 24.8 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.18 us = 0.04% latency, 97.43 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 202.18 us = 0.04% latency, 97.43 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS
                  (default): Linear(512 = 0% Params, 1.41 MMACs = 0% MACs, 124.45 us = 0.03% latency, 22.61 GFLOPS, in_features=4, out_features=128, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (o_proj): lora.Linear(
                809.98 K = 0.16% Params, 2.23 GMACs = 0.19% MACs, 1.09 ms = 0.23% latency, 4.08 TFLOPS
                (base_layer): Linear(802.82 K = 0.16% Params, 2.21 GMACs = 0.19% MACs, 485.18 us = 0.1% latency, 9.09 TFLOPS, in_features=896, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.11 us = 0.05% latency, 89.9 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 219.11 us = 0.05% latency, 89.9 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 129.46 us = 0.03% latency, 152.15 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 129.46 us = 0.03% latency, 152.15 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
            )
            (mlp): Qwen2MLP(
              13.14 M = 2.65% Params, 36.12 GMACs = 3.14% MACs, 9.55 ms = 2.04% latency, 7.56 TFLOPS
              (gate_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.94 ms = 0.63% latency, 8.2 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.21 ms = 0.47% latency, 10.82 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.58 us = 0.05% latency, 91.8 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.58 us = 0.05% latency, 91.8 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.82 us = 0.04% latency, 581.71 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 183.82 us = 0.04% latency, 581.71 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (up_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 2.95 ms = 0.63% latency, 8.16 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.22 ms = 0.47% latency, 10.78 TFLOPS, in_features=896, out_features=4864, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.82 us = 0.05% latency, 91.7 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 214.82 us = 0.05% latency, 91.7 GFLOPS, in_features=896, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.25 us = 0.04% latency, 577.22 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 185.25 us = 0.04% latency, 577.22 GFLOPS, in_features=4, out_features=4864, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (down_proj): lora.Linear(
                4.38 M = 0.88% Params, 12.04 GMACs = 1.05% MACs, 3.32 ms = 0.71% latency, 7.26 TFLOPS
                (base_layer): Linear(4.36 M = 0.88% Params, 11.98 GMACs = 1.04% MACs, 2.16 ms = 0.46% latency, 11.07 TFLOPS, in_features=4864, out_features=896, bias=False)
                (lora_dropout): ModuleDict(
                  0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS
                  (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.75 us = 0.01% latency, 0 FLOPS)
                )
                (lora_A): ModuleDict(
                  19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 780.58 us = 0.17% latency, 136.99 GFLOPS
                  (default): Linear(19.46 K = 0% Params, 53.47 MMACs = 0% MACs, 780.58 us = 0.17% latency, 136.99 GFLOPS, in_features=4864, out_features=4, bias=False)
                )
                (lora_B): ModuleDict(
                  3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS
                  (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 124.22 us = 0.03% latency, 158.58 GFLOPS, in_features=4, out_features=896, bias=False)
                )
                (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
                (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
              )
              (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.02% latency, 128 GFLOPS)
            )
            (input_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 332.83 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 322.58 us = 0.07% latency, 0 FLOPS, (896,), eps=1e-06)
          )
        )
        (norm): Qwen2RMSNorm(896 = 0% Params, 0 MACs = 0% MACs, 280.14 us = 0.06% latency, 0 FLOPS, (896,), eps=1e-06)
        (rotary_emb): Qwen2RotaryEmbedding(0 = 0% Params, 879.36 KMACs = 0% MACs, 647.31 us = 0.14% latency, 2.72 GFLOPS)
      )
      (value_head): lora.Linear(
        4.48 K = 0% Params, 12.32 MMACs = 0% MACs, 704.29 us = 0.15% latency, 34.99 GFLOPS
        (base_layer): Linear(896 = 0% Params, 2.46 MMACs = 0% MACs, 154.02 us = 0.03% latency, 31.97 GFLOPS, in_features=896, out_features=1, bias=False)
        (lora_dropout): ModuleDict(
          0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS
          (default): Identity(0 = 0% Params, 0 MACs = 0% MACs, 25.51 us = 0.01% latency, 0 FLOPS)
        )
        (lora_A): ModuleDict(
          3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 185.73 us = 0.04% latency, 106.06 GFLOPS
          (default): Linear(3.58 K = 0% Params, 9.85 MMACs = 0% MACs, 185.73 us = 0.04% latency, 106.06 GFLOPS, in_features=896, out_features=4, bias=False)
        )
        (lora_B): ModuleDict(
          4 = 0% Params, 10.99 KMACs = 0% MACs, 115.87 us = 0.02% latency, 189.73 MFLOPS
          (default): Linear(4 = 0% Params, 10.99 KMACs = 0% MACs, 115.87 us = 0.02% latency, 189.73 MFLOPS, in_features=4, out_features=1, bias=False)
        )
        (lora_embedding_A): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
        (lora_embedding_B): ParameterDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
        (lora_magnitude_vector): ModuleDict(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
      )
    )
  )
)
------------------------------------------------------------------------------
